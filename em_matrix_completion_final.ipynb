{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "offensive-theme",
   "metadata": {},
   "source": [
    "# EXPECTATION-MAXIMIZATION ALGORITHM FOR INCOMPLETE MATRIX WITH MATRIX COMPLETION,  RMSE COMPUTATION AND BIC CALCULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-swift",
   "metadata": {},
   "source": [
    "## 0. Import Packages & Set up Gaussian Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "practical-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Circle, Arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "enormous-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture(NamedTuple):\n",
    "    \"\"\"Tuple holding a gaussian mixture\"\"\"\n",
    "    mu: np.ndarray  # (K, d) array - each row corresponds to a gaussian component mean\n",
    "    var: np.ndarray  # (K, ) array - each row corresponds to the variance of a component\n",
    "    p: np.ndarray  # (K, ) array = each row corresponds to the weight of a component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-precipitation",
   "metadata": {},
   "source": [
    "## 1. estep: Compute Posterior Probabilities and Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "whole-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estep_em(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"E-step: Softly assigns each datapoint to a gaussian component\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        mixture: the current gaussian mixture\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the assignment\n",
    "\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K, _ = mixture.mu.shape\n",
    "    mu, var, pi = mixture\n",
    "    identifier = np.where(X>=1, 1, 0)\n",
    "    post_log = np.zeros((N, K))\n",
    "\n",
    "    for k in range(K):\n",
    "        post_log[:,k] = np.sum(np.multiply(np.log(norm.pdf(X,mu[k],var[k]**0.5)),identifier),axis=1)\n",
    "\n",
    "    # soft counts\n",
    "    # p(point x(i) was generated by cluster j|x(i),θ)≜p(j∣i)=pjN(x(i);μ(j),σ2jI)p(x(i)∣θ).\n",
    "    pi_log = np.log(pi)\n",
    "    r_log = (pi_log+post_log)-logsumexp(pi_log+post_log, axis=1)[:,None]\n",
    "    r = np.exp(r_log)\n",
    "    \n",
    "    # log MLE\n",
    "    # ℓ^(x(1),…,x(n)∣θ)≜∑i=1n∑j=1Kp(j∣i)log(p(x(i) and x(i) generated by cluster j∣θ)p(j∣i)).    \n",
    "    l = np.sum((pi_log+post_log-r_log)*r)\n",
    "    \n",
    "    return r,l\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-bangladesh",
   "metadata": {},
   "source": [
    "## 2. mstep: Compute the Optimized Parameters of the GaussianMixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "meaning-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mstep_em(X: np.ndarray, post: np.ndarray, mixture: GaussianMixture,\n",
    "          min_variance: float = .25) -> GaussianMixture:\n",
    "    \"\"\"M-step: Updates the gaussian mixture by maximizing the log-likelihood\n",
    "    of the weighted dataset\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        mixture: the current gaussian mixture\n",
    "        min_variance: the minimum variance for each gaussian\n",
    "\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "    \"\"\"\n",
    "    r = post\n",
    "    N, D = X.shape\n",
    "    K = r.shape[1]       \n",
    "    identifier = np.where(X>=1, 1, 0)\n",
    "    Cu_count = np.sum(identifier,axis=1)\n",
    "    \n",
    "   # mean (K,D) - -only update if denominator >=1\n",
    "    mean = mixture[0]\n",
    "    new_mean = np.where(np.dot(r.T,X)/(np.dot(r.T,identifier)+1e-16)<np.dot(r.T,X),np.dot(r.T,X)/(np.dot(r.T,identifier)+1e-16),mean)\n",
    "\n",
    "    # var (K,) -- only update if min_var > 0.25  \n",
    "    new_var = np.zeros((1,K))    \n",
    "    for k in range(K):\n",
    "        var_numerator = np.dot(r[:,k].T,np.sum(np.where(np.isnan(np.multiply(identifier,(X-new_mean[k,:])**2))==True,0,np.multiply(identifier,(X-new_mean[k,:])**2)),axis=1))\n",
    "        var_denominator = np.dot(r[:,k].T,Cu_count)\n",
    "        new_var[0,k] = var_numerator/var_denominator        \n",
    "    new_var = np.array([max(min_variance,i) for i in new_var[0]])\n",
    "\n",
    "    # weights (K,) -- (4,) array\n",
    "    new_pi = r.sum(axis = 0)/N\n",
    "    \n",
    "    return GaussianMixture(new_mean,new_var,new_pi)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-glory",
   "metadata": {},
   "source": [
    "## 3. Run the estep and mstep until the Absolute Optimization is Reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "israeli-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(X: np.ndarray, mixture: GaussianMixture,\n",
    "        post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:\n",
    "    \"\"\"Runs the mixture model\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the current assignment\n",
    "    \"\"\"\n",
    "    prev_l = None\n",
    "    l = None\n",
    "    while (prev_l is None or  l - prev_l > 1e-6*np.absolute(l)):\n",
    "        prev_l = l\n",
    "        r, l = estep_em(X, mixture)\n",
    "        mixture = mstep_em(X, r, mixture, 0.25)\n",
    "\n",
    "    return mixture, r, l\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-boundary",
   "metadata": {},
   "source": [
    "## 4. Fill the Incomplete Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "worthy-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_matrix(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:\n",
    "    \"\"\"Fills an incomplete matrix according to a mixture model\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array of incomplete data (incomplete entries =0)\n",
    "        mixture: a mixture of gaussians\n",
    "\n",
    "    Returns\n",
    "        np.ndarray: a (n, d) array with completed data\n",
    "    \"\"\"\n",
    "    X1 = X.copy()  \n",
    "    N, D = X.shape\n",
    "    K, _ = mixture.mu.shape\n",
    "    mu, var, pi = mixture\n",
    "    identifier = np.where(X>=1, 1, 0)\n",
    "    post_log = np.zeros((N, K))\n",
    "\n",
    "    for k in range(K):\n",
    "        post_log[:,k] = np.sum(np.multiply(np.log(norm.pdf(X,mu[k],var[k]**0.5)),identifier),axis=1)\n",
    "\n",
    "    pi_log = np.log(pi)\n",
    "    r_log = (pi_log+post_log)-logsumexp(pi_log+post_log, axis=1)[:,None]\n",
    "    r = np.exp(r_log)\n",
    "    \n",
    "    for n in range(N):\n",
    "        hu = [j[0]  for j in list(enumerate(X1[n,:])) if j[1] ==0]\n",
    "        for i in hu:\n",
    "            X1[n,i] = np.dot(r[n,:],mu[:,i])\n",
    "            \n",
    "    return X1        \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-tender",
   "metadata": {},
   "source": [
    "## 5. RMSE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "involved-arctic",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-268-5538f723e26a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(X, X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-zambia",
   "metadata": {},
   "source": [
    "## 6. BIC Calculatioin for K Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(X: np.ndarray, mixture: GaussianMixture,\n",
    "        log_likelihood: float) -> float:\n",
    "    \"\"\"Computes the Bayesian Information Criterion for a\n",
    "    mixture of gaussians\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        mixture: a mixture of spherical gaussian\n",
    "        log_likelihood: the log-likelihood of the data\n",
    "\n",
    "    Returns:\n",
    "        float: the BIC for this mixture\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    K, D = mixture.mu.shape \n",
    "    \n",
    "    # free parameters = K(#pi) + K(#var) + K*D(#mu) - 1\n",
    "    \n",
    "    p = K+K+K*D-1    \n",
    "    BIC = log_likelihood - p*np.log(N)/2\n",
    "    \n",
    "    return BIC\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
